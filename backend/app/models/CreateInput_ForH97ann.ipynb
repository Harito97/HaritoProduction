{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "!git clone https://github.com/Harito97/HaritoProduction\n",
    "%cd ThyroidCancerClassifier\n",
    "%ls\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() # 00e43aa49b2b8c17d0db66c858191c6420f4dc9e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.app.models.H97 import H97_ANN\n",
    "from backend.app.app import App\n",
    "model1_path = '/kaggle/input/thyroidcancerclassifier/H97_ANN.pth'\n",
    "model2_path = '/kaggle/input/thyroidcancerclassifier/H97_ANN.pth'\n",
    "app = App(model1_path, model2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '/kaggle/input/thyroidcancer-ver1/dataver1/ver1'\n",
    "input_info = {}\n",
    "for dataset in ['train', 'valid', 'test']:\n",
    "    results = []\n",
    "    labels = []\n",
    "    for index_label, label in enumerate(['.B2', 'B5', 'B6']):\n",
    "        for filename in os.listdir(f'{data_dir}/{dataset}/{label}'):\n",
    "            if filename.endswith('.jpg'):\n",
    "                image_path = f'{data_dir}/{dataset}/{label}/{filename}'\n",
    "            else:\n",
    "                continue\n",
    "            print(f'Processing {image_path}')\n",
    "            results.append(app.get_images(image_path))\n",
    "            labels.append([index_label] * 18)\n",
    "    input_info[dataset] = {'results': results, 'labels': labels}\n",
    "\n",
    "# Save input_info\n",
    "import json\n",
    "with open('input_info.json', 'w') as f:\n",
    "    json.dump(input_info, f)\n",
    "# Read input_info\n",
    "with open('input_info.json', 'r') as f:\n",
    "    input_info = json.load(f)\n",
    "# Get train dataset\n",
    "train_dataset = input_info['train']\n",
    "# Get valid dataset\n",
    "valid_dataset = input_info['valid']\n",
    "# Get test dataset\n",
    "test_dataset = input_info['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import wandb\n",
    "\n",
    "class Tool:\n",
    "    @staticmethod\n",
    "    def save_confusion_matrix(y_true, y_score, target_names, filename, normalize=False):\n",
    "        \"\"\"\n",
    "        Saves the confusion matrix to a file.\n",
    "\n",
    "        Args:\n",
    "            y_true (array-like): True labels.\n",
    "            y_score (array-like): Predicted scores.\n",
    "            target_names (list): Names of the target classes.\n",
    "            filename (str): Path to save the confusion matrix.\n",
    "            normalize (bool, optional): Whether to normalize the confusion matrix. Defaults to False.\n",
    "        Returns:\n",
    "            cm (numpy.ndarray): The confusion matrix.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cm = confusion_matrix(y_true, y_score)\n",
    "            print(\"Confusion Matrix:\\n\", cm)\n",
    "            if normalize:\n",
    "                cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "                title = \"Normalized Confusion Matrix\"\n",
    "            else:\n",
    "                title = \"Confusion Matrix, Without Normalization\"\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(\n",
    "                cm,\n",
    "                annot=True,\n",
    "                fmt=\".2f\" if normalize else \"d\",\n",
    "                cmap=\"Blues\",\n",
    "                xticklabels=target_names,\n",
    "                yticklabels=target_names,\n",
    "            )\n",
    "            plt.ylabel(\"True label\")\n",
    "            plt.xlabel(\"Predicted label\")\n",
    "            plt.title(title)\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            return cm\n",
    "        except ValueError as e:\n",
    "            print(f\"Error creating confusion matrix: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_classification_report(y_true, y_score, filename):\n",
    "        \"\"\"\n",
    "        Saves the classification report to a file.\n",
    "\n",
    "        Args:\n",
    "            y_true (array-like): True labels.\n",
    "            y_score (array-like): Predicted scores.\n",
    "            filename (str): Path to save the classification report.\n",
    "        Returns:\n",
    "            cr (dict): The classification report.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cr = classification_report(y_true, y_score, output_dict=True)\n",
    "            print(\"Classification Report:\\n\", cr)\n",
    "            report_df = pd.DataFrame(cr).transpose()\n",
    "            report_df.drop(\n",
    "                \"support\", axis=1, inplace=True\n",
    "            )  # Bỏ cột support nếu không cần\n",
    "            report_df.plot(kind=\"bar\", figsize=(10, 6))\n",
    "            plt.title(\"Classification Report\")\n",
    "            plt.ylabel(\"Score\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            return cr\n",
    "        except ValueError as e:\n",
    "            print(f\"Error creating DataFrame from classification report: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_roc_auc_plot(y_true, y_score, n_classes, filename):\n",
    "        \"\"\"\n",
    "        Calculates and saves the ROC AUC plot to a file.\n",
    "\n",
    "        Args:\n",
    "            y_true (array-like): True labels.\n",
    "            y_score (array-like): Predicted scores.\n",
    "            n_classes (int): Number of classes.\n",
    "            filename (str): Path to save the plot.\n",
    "        Returns:\n",
    "            fpr (dict): False positive rates for each class.\n",
    "            tpr (dict): True positive rates for each class.\n",
    "            roc_auc (dict): ROC AUC scores for each class.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert y_true and y_score to NumPy arrays if they are lists\n",
    "            y_true = np.array(y_true)\n",
    "            y_score = np.array(y_score)\n",
    "\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "\n",
    "            # Binarize the output if more than 2 classes\n",
    "            if n_classes > 2:\n",
    "                y_true = label_binarize(y_true, classes=[*range(n_classes)])\n",
    "                for i in range(n_classes):\n",
    "                    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
    "                    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            else:\n",
    "                fpr[1], tpr[1], _ = roc_curve(y_true, y_score[:, 1])\n",
    "                roc_auc[1] = auc(fpr[1], tpr[1])\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            if n_classes == 2:\n",
    "                plt.plot(\n",
    "                    fpr[1],\n",
    "                    tpr[1],\n",
    "                    lw=2,\n",
    "                    label=\"ROC curve (area = {0:0.2f})\".format(roc_auc[1]),\n",
    "                )\n",
    "            else:\n",
    "                for i in range(n_classes):\n",
    "                    plt.plot(\n",
    "                        fpr[i],\n",
    "                        tpr[i],\n",
    "                        lw=2,\n",
    "                        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(\n",
    "                            i, roc_auc[i]\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            return fpr, tpr, roc_auc\n",
    "        except ValueError as e:\n",
    "            print(f\"Error creating ROC AUC plot: {e}\")\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "\n",
    "# Số lượng mẫu cho từng lớp\n",
    "num_samples_B2 = 103\n",
    "num_samples_B5 = 541\n",
    "num_samples_B6 = 777\n",
    "\n",
    "# Tổng số mẫu\n",
    "total_samples = num_samples_B2 + num_samples_B5 + num_samples_B6\n",
    "num_classes = 3\n",
    "\n",
    "# Tính trọng số cho từng lớp\n",
    "class_weight_B2 = total_samples / (num_classes * num_samples_B2)\n",
    "class_weight_B5 = total_samples / (num_classes * num_samples_B5)\n",
    "class_weight_B6 = total_samples / (num_classes * num_samples_B6)\n",
    "\n",
    "# Tạo tensor chứa trọng số\n",
    "model = H97_ANN()\n",
    "class_weights = torch.tensor([class_weight_B2, class_weight_B5, class_weight_B6])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = criterion.to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Def to train the H97_ANN model\n",
    "def train():\n",
    "    batch_size = 32\n",
    "    epochs = 100\n",
    "    patience = 20\n",
    "    wandb.init(\n",
    "        project=\"ThyroidCancer\",\n",
    "        entity=\"harito\",\n",
    "        name=\"H97_ANN_train\",\n",
    "        config={\"batch_size\": batch_size, \"epochs\": epochs, \"patience\": patience},\n",
    "    )\n",
    "    step = train_dataset[\"results\"][0].shape[1] // batch_size\n",
    "\n",
    "    history_file_path = f\"history.json\"\n",
    "    model_best = f\"model_best.pt\"\n",
    "    model_last = f\"model_last.pt\"\n",
    "\n",
    "    print(\"Initializing history ...\")\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    print(\"Training classification model...\")\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    for i in range(epochs):\n",
    "        running_loss = 10 ** 10\n",
    "        train_preds = np.array([])\n",
    "        train_targets = np.array([])\n",
    "        preds = torch.tensor([])\n",
    "        for j in range(step):\n",
    "            if j * batch_size >= len(train_dataset[\"results\"]):\n",
    "                x = train_dataset[\"results\"][j * batch_size :]\n",
    "                y = train_dataset[\"labels\"][j * batch_size :]\n",
    "            else:\n",
    "                x = train_dataset[\"results\"][j * batch_size : (j + 1) * batch_size]\n",
    "                y = train_dataset[\"labels\"][j * batch_size : (j + 1) * batch_size]\n",
    "\n",
    "            loss, output = model.fix_batch(x, y, criterion, optimizer)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            train_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            train_targets.extend(y.view(-1).cpu().numpy())\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset[\"results\"])\n",
    "        train_acc = np.mean(\n",
    "            np.array(preds.view(-1).cpu().numpy()) == np.array(train_dataset[\"labels\"])\n",
    "        )\n",
    "        train_f1 = f1_score(\n",
    "            train_dataset[\"labels\"], preds.view(-1).cpu().numpy(), average=\"weighted\"\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "\n",
    "        valid_loss, valid_output = model.fix_batch(\n",
    "            valid_dataset[\"results\"], valid_dataset[\"labels\"]\n",
    "        )\n",
    "        val_loss = valid_loss.item() / len(valid_dataset[\"results\"])\n",
    "        _, preds = torch.max(valid_output, 1)\n",
    "        val_acc = np.mean(\n",
    "            np.array(preds.view(-1).cpu().numpy()) == np.array(valid_dataset[\"labels\"])\n",
    "        )\n",
    "        val_f1 = f1_score(\n",
    "            valid_dataset[\"labels\"], preds.view(-1).cpu().numpy(), average=\"weighted\"\n",
    "        )\n",
    "\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1\": val_f1,\n",
    "                epoch: i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(history_file_path, \"w\") as history_file:\n",
    "            json.dump(history, history_file)\n",
    "\n",
    "        if val_loss > best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_best)\n",
    "            print(f\"Model saved with val_loss: {val_loss}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            torch.save(model.state_dict(), model_last)\n",
    "            print(f\"Patience counter: {patience_counter}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Loading the best model ...\")\n",
    "    model.load_state_dict(torch.load(\"model_best.pt\"))\n",
    "    model.eval()\n",
    "    test_loss, test_output = model.fix_batch(\n",
    "        test_dataset[\"results\"], test_dataset[\"labels\"]\n",
    "    )\n",
    "    _, preds = torch.max(test_output, 1)\n",
    "    test_loss = test_loss.item() / len(test_dataset[\"results\"])\n",
    "    test_acc = np.mean(\n",
    "        np.array(preds.view(-1).cpu().numpy()) == np.array(test_dataset[\"labels\"])\n",
    "    )\n",
    "    test_f1 = f1_score(\n",
    "        test_dataset[\"labels\"], preds.view(-1).cpu().numpy(), average=\"weighted\"\n",
    "    )\n",
    "    # Save confusion matrix\n",
    "    Tool.save_confusion_matrix(\n",
    "        test_dataset[\"labels\"],\n",
    "        preds.view(-1).cpu().numpy(),\n",
    "        [\"B2\", \"B5\", \"B6\"],\n",
    "        \"confusion_matrix.png\",\n",
    "    )\n",
    "    # Save classification report\n",
    "    Tool.save_classification_report(\n",
    "        test_dataset[\"labels\"],\n",
    "        preds.view(-1).cpu().numpy(),\n",
    "        \"classification_report.png\",\n",
    "    )\n",
    "    # Save ROC AUC plot\n",
    "    Tool.save_roc_auc_plot(\n",
    "        test_dataset[\"labels\"], preds.view(-1).cpu().numpy(), 3, \"roc_auc.png\"\n",
    "    )\n",
    "\n",
    "    print(f\"Test loss: {test_loss}, Test accuracy: {test_acc}, Test f1: {test_f1}\")\n",
    "    wandb.init(\n",
    "        project=\"ThyroidCancer\",\n",
    "        entity=\"harito\",\n",
    "        name=\"H97_ANN_test\",\n",
    "        config={\"test_loss\": test_loss, \"test_acc\": test_acc, \"test_f1\": test_f1},\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"confusion_matrix\": wandb.Image(\n",
    "                f\"confusion_matrix.png\"\n",
    "            ),\n",
    "            \"classification_report\": wandb.Image(\n",
    "                f\"classification_report.png\"\n",
    "            ),\n",
    "            \"roc_auc_plot\": wandb.Image(f\"roc_auc_plot.png\"),\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
